{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f181180a",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dd0d81",
   "metadata": {},
   "source": [
    "# INSTRUCTIONS (QUESTION 1 and 2)\n",
    "\n",
    "For Questions 1 and 2, you are asked to perform the following tasks based on the following target website, which contains artificial content designed for this assignment: https://sitescrape.awh.durham.ac.uk/comp42315_resit/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7f40b",
   "metadata": {},
   "source": [
    "## Question 1 (35 marks)\n",
    "\n",
    "Please design and implement a solution to crawl the publication title, year and author list of every unique publication record on the target website. Then, using Pandas DataFrame, please create and display a table that contains these unique records. The table should consist of five columns: the row number in the table, publication title, author list, year, and the number of authors (hint: you will need to develop an algorithm to work this out). The records in the table should be sorted first according to the descending number of author values, then by the descending number of year values, and finally by the titles from A to Z. Include the full table in your Jupyter Notebook. \n",
    "\n",
    "[Explain your design and highlight any features in this questionâ€™s report part of your Jupyter Notebook in no more than 300 words. ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e813fb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row number.</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Authors</th>\n",
       "      <th>No. Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>Single Sketch Image based 3D Car Shape Reconstruction with Deep Learning and Lazy Learning</td>\n",
       "      <td>2021</td>\n",
       "      <td>Yue Ma,, Jing Guo, Zili Zhang, Yunfei Li, Frederick W. B. Li, Hubert P. H. Shum, Bailin Yang, Xiaohui Liang</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2</td>\n",
       "      <td>Two-Stage Human Verification using HandCAPTCHA and Anti-Spoofed Finger Biometrics with Feature Selection</td>\n",
       "      <td>2021</td>\n",
       "      <td>Daniel Organisciak, Matthew Poyser, Aishah Alsehaim, Shanfeng Hu, Brian K. S. Isaac-Medina, Toby P. Breckon, Hubert P. H. Shum</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3</td>\n",
       "      <td>3D Car Shape Reconstruction from a Contour Sketch using GAN and Lazy Learning</td>\n",
       "      <td>2023</td>\n",
       "      <td>Edmond S. L. Ho, Arindam Kar, Sourav Pramanik, Arghya Chakraborty, Debotosh Bhattacharjee, Hubert P. H. Shum</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4</td>\n",
       "      <td>DSPP: Deep Shape and Pose Priors of Humans</td>\n",
       "      <td>2021</td>\n",
       "      <td>Li Yi,, Edmond S. L. Ho, Jacky C. P. Chan, Hubert P. H. Shum, He Wang, Wei Wei</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5</td>\n",
       "      <td>Resolving Hand-Object Occlusion for Mixed Reality with Joint Deep Learning and Model Optimization</td>\n",
       "      <td>2021</td>\n",
       "      <td>Edmond S. L. Ho,, Kevin D. McCay, Hubert P. H. Shum, Gerhard Fehringer, Claire Marcroft, Nicholas Embleton</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>6</td>\n",
       "      <td>A Unified Deep Metric Representation for Mesh Saliency Detection and Non-rigid Shape Matching</td>\n",
       "      <td>2020</td>\n",
       "      <td>Brian K. S. Isaac-Medina, Matthew Poyser, Daniel Organisciak, Chris G. Willcocks, Toby P. Breckon, Hubert P. H. Shum</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>7</td>\n",
       "      <td>CCESK: A Chinese Character Educational System Based on Kinect</td>\n",
       "      <td>2020</td>\n",
       "      <td>Edmond S. L. Ho, Jie Li, Yanpeng Qu, Fei Chao, Hubert P. H. Shum, Longzhi Yang</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>A Generic Framework for Editing and Synthesizing Multimodal Data with Relative Emotion Strength</td>\n",
       "      <td>2019</td>\n",
       "      <td>Edmond S. L. Ho,, Hubert P. H. Shum, Marie-Paule Cani, Tiberiu Popa, Daniel Holden, He Wang</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9</td>\n",
       "      <td>Identifying Abnormal Gait in Older People during Multiple-Tasks Assessment with Audio-Visual Cues</td>\n",
       "      <td>2018</td>\n",
       "      <td>Jiao Li, Lanling Zeng,, Yang Yang, Howard Leung, Hubert P. H. Shum, Nauman Aslam, Zhigeng Pan</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>10</td>\n",
       "      <td>NETIVAR: NETwork Information Visualization based on Augmented Reality</td>\n",
       "      <td>2018</td>\n",
       "      <td>Alan Godfrey, Victoria Hetherington, Hubert P. H. Shum, Paolo Bonato, Nigel Lovell, Sam Stuart</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>11</td>\n",
       "      <td>A Motion Classification Approach to Fall Detection</td>\n",
       "      <td>2017</td>\n",
       "      <td>Pierre Plantard, Antoine Muller, Charles Pontonnier, Georges Dumont, Hubert P. H. Shum, Franck Multon</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>12</td>\n",
       "      <td>Towards Sparse Rule Base Generation for Fuzzy Rule Interpolation</td>\n",
       "      <td>2016</td>\n",
       "      <td>Edmond S. L. Ho, Jacky C. P. Chan, Donald C. K. Chan, Hubert P. H. Shum, Yiu-ming Cheung, P. C. Yuen</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>Spatio-temporal Manifold Learning for Human Motions via Long-horizon Modeling</td>\n",
       "      <td>2023</td>\n",
       "      <td>Edmond S. L. Ho, Manli Zhu, Qianhui Men, Hubert P. H. Shum, Howard Leung</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>14</td>\n",
       "      <td>Bi-projection based Foreground-aware Omnidirectional Depth Prediction</td>\n",
       "      <td>2022</td>\n",
       "      <td>Kanglei Zhou, Jiaying Chen, Hubert P. H. Shum, Frederick W. B. Li, Xiaohui Liang</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>15</td>\n",
       "      <td>Foreground-aware Dense Depth Estimation for 360 Images</td>\n",
       "      <td>2022</td>\n",
       "      <td>Munmun Bhattacharya, Sandip Roy, Kamlesh Mistry, Hubert P. H. Shum, Samiran Chattopadhyay</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>16</td>\n",
       "      <td>Human-centric Autonomous Driving in an AV-Pedestrian Interactive Environment Using SVO</td>\n",
       "      <td>2022</td>\n",
       "      <td>Zhiying Leng, Jiaying Chen, Hubert P. H. Shum, Frederick W. B. Li, Xiaohui Liang</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>17</td>\n",
       "      <td>Stable Hand Pose Estimation under Tremor via Graph Neural Network</td>\n",
       "      <td>2022</td>\n",
       "      <td>Asish Bera, Ratnadeep Dey, Debotosh Bhattacharjee, Mita Nasipuri, Hubert P. H. Shum</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>Automatic Sign Dance Synthesis from Gesture-based Sign Language</td>\n",
       "      <td>2021</td>\n",
       "      <td>Edmond S. L. Ho,, Taku Komura, Pengpeng Hu, Nauman Aslam, Hubert P. H. Shum</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>19</td>\n",
       "      <td>Sparse Metric-based Mesh Saliency</td>\n",
       "      <td>2021</td>\n",
       "      <td>Edmond S. L. Ho,, John Hartley, Hubert P. H. Shum, He Wang, Subramanian Ramamoorthy</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>20</td>\n",
       "      <td>A Privacy-Preserving Efficient Location-Sharing Scheme for Mobile Online Social Network Applications</td>\n",
       "      <td>2020</td>\n",
       "      <td>Shanfeng Hu, Hubert P. H. Shum, Nauman Aslam, Frederick W. B. Li, Xiaohui Liang</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>21</td>\n",
       "      <td>Filtering Techniques for Channel Selection in Motor Imagery EEG Applications: A Survey</td>\n",
       "      <td>2020</td>\n",
       "      <td>Shanfeng Hu, Xiaohui Liang, Hubert P. H. Shum, Frederick W. B. Li, Nauman Aslam</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>22</td>\n",
       "      <td>Synthesizing Expressive Facial and Speech Animation by Text-to-IPA Translation with Emotion Control</td>\n",
       "      <td>2020</td>\n",
       "      <td>Edmond S. L. Ho, Taku Komura, Yijun Shen, Joseph Henry, He Wang, Hubert P. H. Shum</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>23</td>\n",
       "      <td>Curvature-Based Sparse Rule Base Generation for Fuzzy Rule Interpolation</td>\n",
       "      <td>2019</td>\n",
       "      <td>Lining Zhang, Hubert P. H. Shum, Li Liu, Guodong Guo, Ling Shao</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>24</td>\n",
       "      <td>Biofeedback Assessment for Older People with Balance Impairment using a Low-cost Balance Board</td>\n",
       "      <td>2018</td>\n",
       "      <td>Ryo Kakitsuka, Kenta Hara, Naoya Iwamoto, Takuya Kato, Hubert P. H. Shum, Shigeo Morishima</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>25</td>\n",
       "      <td>Patient Assessment Assistant Using Augmented Reality</td>\n",
       "      <td>2018</td>\n",
       "      <td>Shanfeng Hu, Hindol Bhattacharya, Matangini Chattopadhyay, Nauman Aslam, Hubert P. H. Shum</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26</td>\n",
       "      <td>Depth Sensor based Facial and Body Animation Control</td>\n",
       "      <td>2017</td>\n",
       "      <td>Huiwen Bian,, Lanling Zeng, Yang Yang, Hubert P. H. Shum, Nauman Aslam</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>27</td>\n",
       "      <td>Discriminative Semantic Subspace Analysis for Relevance Feedback</td>\n",
       "      <td>2017</td>\n",
       "      <td>Shanfeng Hu, Worasak Rueangsirarak, Maxime Bouchee, Nauman Aslam, Hubert P. H. Shum</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>Synthesizing Motion with Relative Emotion Strength</td>\n",
       "      <td>2017</td>\n",
       "      <td>Edmond S. L. Ho,, Yijun Shen, He Wang, Longzhi Yang, Hubert P. H. Shum</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29</td>\n",
       "      <td>Automatic Dance Generation System Considering Sign Language Information</td>\n",
       "      <td>2016</td>\n",
       "      <td>Xin Fu,, Jie Li, Hubert P. H. Shum, Graham Sexton, Longzhi Yang</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>30</td>\n",
       "      <td>Kinect Posture Reconstruction based on a Local Mixture of Gaussian Process Models</td>\n",
       "      <td>2016</td>\n",
       "      <td>Zhiguang Liu, Howard Leung, Liuyang Zhou, Franck Multon, Hubert P. H. Shum</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>31</td>\n",
       "      <td>Real-Time Posture Reconstruction for Microsoft Kinect</td>\n",
       "      <td>2014</td>\n",
       "      <td>Jie Li, Longzhi Yang, Hubert P. H. Shum, Graham Sexton, Yao Tan</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>32</td>\n",
       "      <td>Semantics-STGCNN: A Semantics-guided Spatial Temporal Graph Convolutional Network for Multi-class Trajectory Prediction</td>\n",
       "      <td>2023</td>\n",
       "      <td>Edmond S. L. Ho, Qianhui Men, Hubert P. H. Shum, Howard Leung</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>33</td>\n",
       "      <td>Spoofing Detection on Hand Images Using Quality Assessment</td>\n",
       "      <td>2023</td>\n",
       "      <td>Edmond S. L. Ho,, He Wang, Hubert P. H. Shum, Zhanxing Zhu</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>34</td>\n",
       "      <td>Abnormal Infant Movements Classification with Deep Learning on Pose-based Features</td>\n",
       "      <td>2022</td>\n",
       "      <td>Edmond S. L. Ho, Yijun Shen, Longzhi Yang, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>35</td>\n",
       "      <td>DurLAR: A High-fidelity 128-channel LiDAR Dataset with Panoramic Ambientand Reflectivity Imagery for Multi-modal Autonomous Driving Applications</td>\n",
       "      <td>2022</td>\n",
       "      <td>Edmond S. L. Ho,, Dimitrios Sakkos, Hubert P. H. Shum, Garry Elvin</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>36</td>\n",
       "      <td>Formation Control for UAVs Using a Flux Guided Approach</td>\n",
       "      <td>2022</td>\n",
       "      <td>Li Li, Khalid N. Ismail, Hubert P. H. Shum, Toby P. Breckon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>GAN-based Reactive Motion Synthesis with Class-aware Discriminators for Human-human Interaction</td>\n",
       "      <td>2022</td>\n",
       "      <td>Edmond S. L. Ho,, Qianhui Men, Hubert P. H. Shum, Howard Leung</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>38</td>\n",
       "      <td>An Interactive Motion Analysis Framework for Diagnosing and Rectifying Potential Injuries Caused Through Resistance Training</td>\n",
       "      <td>2021</td>\n",
       "      <td>Naoya Iwamoto, Hubert P. H. Shum, Wakana Asahina, Shigeo Morishima</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>39</td>\n",
       "      <td>LMZMPM: Local Modified Zernike Moment Per-unit Mass for Robust Human Face Recognition</td>\n",
       "      <td>2021</td>\n",
       "      <td>Edmond S. L. Ho, Luca Crosato, Chongfeng Wei, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>40</td>\n",
       "      <td>STGAE: Spatial Temporal Graph Auto-encoder for Hand Motion Denoising</td>\n",
       "      <td>2021</td>\n",
       "      <td>Edmond S. L. Ho, Naoki Nozawa, Hubert P. H. Shum, Qi Feng</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>41</td>\n",
       "      <td>Triplet Loss with Channel Attention for Person Re-identification</td>\n",
       "      <td>2021</td>\n",
       "      <td>Subhas Barman, Hubert P. H. Shum, Samiran Chattopadhyay, Debasis Samanta</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>42</td>\n",
       "      <td>A Dual-Stream Recurrent Neural Network for Student Feedback Prediction using Kinect</td>\n",
       "      <td>2020</td>\n",
       "      <td>Worasak Rueangsirarak, Jingtian Zhang, Nauman Aslam, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>43</td>\n",
       "      <td>A New Method to Evaluate the Dynamic Air Gap Thickness and Garment Sliding of Virtual Clothes During Walking</td>\n",
       "      <td>2020</td>\n",
       "      <td>Edmond S. L. Ho, Naoki Nozawa, Hubert P. H. Shum, Shigeo Morishima</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>44</td>\n",
       "      <td>Data-Driven Crowd Motion Control with Multi-touch Gestures</td>\n",
       "      <td>2020</td>\n",
       "      <td>Edmond S. L. Ho, Jake Hall, Jacky C. P. Chan, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>45</td>\n",
       "      <td>Multi-task Deep Learning with Optical Flow Features for Self-Driving Cars</td>\n",
       "      <td>2020</td>\n",
       "      <td>Edmond S. L. Ho, Ying Huang, Hubert P. H. Shum, Nauman Aslam</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>46</td>\n",
       "      <td>Unifying Person and Vehicle Re-identification</td>\n",
       "      <td>2020</td>\n",
       "      <td>Edmond S. L. Ho,, Marie-Paule Cani, Tiberiu Popa, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47</td>\n",
       "      <td>DanceDJ: A 3D Dance Animation Authoring System for Live Performance</td>\n",
       "      <td>2019</td>\n",
       "      <td>Muhammad Zeeshan Baig, Nauman Aslam, Hubert P. H. Shum, Li Zhang</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>48</td>\n",
       "      <td>Manifold Regularized Experimental Design for Active Learning</td>\n",
       "      <td>2019</td>\n",
       "      <td>Zheming Zuo, Daniel Organisciak, Hubert P. H. Shum, Longzhi Yang</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>49</td>\n",
       "      <td>Prior-less 3D Human Shape Reconstruction with an Earth Mover's Distance Informed CNN</td>\n",
       "      <td>2019</td>\n",
       "      <td>Daniel Organisciak, Chirine Riachy, Nauman Aslam, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>50</td>\n",
       "      <td>Resolving Occlusion for 3D Object Manipulation with Hands in Mixed Reality</td>\n",
       "      <td>2019</td>\n",
       "      <td>Edmond S. L. Ho, Christopher Flinton, Philip Anderson, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>51</td>\n",
       "      <td>Action Recognition from Arbitrary Views Using Transferable Dictionary Learning</td>\n",
       "      <td>2018</td>\n",
       "      <td>Edmond S. L. Ho, Naoki Nozawa, Hubert P. H. Shum, Shigeo Morishima</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>52</td>\n",
       "      <td>Automatic Musculoskeletal and Neurological Disorder Diagnosis with Relative Joint Displacement from Human Gait</td>\n",
       "      <td>2018</td>\n",
       "      <td>Edmond S. L. Ho, Jingtian Zhang, Hubert P. H. Shum, Kevin D. McCay</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>53</td>\n",
       "      <td>Occlusion for 3D Object Manipulation with Hands in Augmented Reality</td>\n",
       "      <td>2018</td>\n",
       "      <td>Edmond S. L. Ho, Andreea Stef, Kaveen Perera, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>54</td>\n",
       "      <td>Saliency-Informed Spatio-Temporal Vector of Locally Aggregated Descriptors and Fisher Vectors for Visual Action Recognition</td>\n",
       "      <td>2018</td>\n",
       "      <td>Zhiguang Liu, Liuyang Zhou, Howard Leung, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>55</td>\n",
       "      <td>SkillVis: A Visualization Tool for Boxing Skill Assessment</td>\n",
       "      <td>2018</td>\n",
       "      <td>Zhiguang Liu, Liuyang Zhou, Howard Leung, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>56</td>\n",
       "      <td>Temporal Clustering of Motion Capture Data with Optimal Partitioning</td>\n",
       "      <td>2017</td>\n",
       "      <td>Jingtian Zhang, Lining Zhang, Hubert P. H. Shum, Ling Shao</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>57</td>\n",
       "      <td>Usability of Corrected Kinect Measurement for Ergonomic Evaluation in Constrained Environment</td>\n",
       "      <td>2017</td>\n",
       "      <td>Pierre Plantard, Hubert P. H. Shum, Anne-Sophie Le Pierres, Franck Multon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>58</td>\n",
       "      <td>Validation of an Ergonomic Assessment Method using Kinect Data in Real Workplace Conditions</td>\n",
       "      <td>2017</td>\n",
       "      <td>Edmond S. L. Ho, Kevin D. McCay, Hubert P. H. Shum, Longzhi Yang</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>59</td>\n",
       "      <td>Coordinated Crowd Simulation with Topological Scene Analysis</td>\n",
       "      <td>2016</td>\n",
       "      <td>Edmond S. L. Ho,, Shoujiang Xu, Nauman Aslam, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>60</td>\n",
       "      <td>Multi-layer Lattice Model for Real-Time Dynamic Character Deformation</td>\n",
       "      <td>2016</td>\n",
       "      <td>Jie Li, Yanpeng Qu, Hubert P. H. Shum, Longzhi Yang</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>61</td>\n",
       "      <td>An Intelligent Mobile-Based Automatic Diagnostic System to Identify Retinal Diseases using Mathematical Morphological Operations</td>\n",
       "      <td>2015</td>\n",
       "      <td>Zhiguang Liu, Howard Leung, Liuyang Zhou, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>62</td>\n",
       "      <td>Motion Adaptation for Humanoid Robots in Constrained Environments</td>\n",
       "      <td>2015</td>\n",
       "      <td>Liuyang Zhou, Zhiguang Liu, Howard Leung, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>63</td>\n",
       "      <td>Environment-aware Real-Time Crowd Control</td>\n",
       "      <td>2014</td>\n",
       "      <td>Edmond S. L. Ho, Taku Komura, Hubert P. H. Shum, Ludovic Hoyet, Franck Multon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64</td>\n",
       "      <td>Human Motion Variation Synthesis with Multivariate Gaussian Processes</td>\n",
       "      <td>2014</td>\n",
       "      <td>Yijun Shen, Jingtian Zhang, Longzhi Yang, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>65</td>\n",
       "      <td>Posture Reconstruction Using Kinect with a Probabilistic Model</td>\n",
       "      <td>2014</td>\n",
       "      <td>Naoya Iwamoto, Hubert P. H. Shum, Longzhi Yang, Shigeo Morishima</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>66</td>\n",
       "      <td>Preparation Behaviour Synthesis with Reinforcement Learning</td>\n",
       "      <td>2014</td>\n",
       "      <td>Mohamed Omar, Alamgir Hossain, Li Zhang, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>67</td>\n",
       "      <td>Natural Preparation Behavior Synthesis</td>\n",
       "      <td>2013</td>\n",
       "      <td>Liuyang Zhou, Lifeng Shang, Hubert P. H. Shum, Howard Leung</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>68</td>\n",
       "      <td>Serious Games with Human-Object Interactions using RGB-D Camera</td>\n",
       "      <td>2013</td>\n",
       "      <td>Edmond S. L. Ho,, Hubert P. H. Shum, Yang Jiang, Shu Takagi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>69</td>\n",
       "      <td>Simulating Multiple Character Interactions with Collaborative and Adversarial Goals</td>\n",
       "      <td>2013</td>\n",
       "      <td>Edmond S. L. Ho, Hubert P. H. Shum, Yiu-ming Cheung, P. C. Yuen</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>70</td>\n",
       "      <td>Environment Capturing with Microsoft Kinect</td>\n",
       "      <td>2012</td>\n",
       "      <td>Edmond S. L. Ho, Taku Komura, Hubert P. H. Shum, Ludovic Hoyet, Franck Multon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>71</td>\n",
       "      <td>Emulating Human Perception of Motion Similarity</td>\n",
       "      <td>2009</td>\n",
       "      <td>Taku Komura,, Hubert P. H. Shum, Takaaki Shiratori, Shu Takagi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>72</td>\n",
       "      <td>Simulating Competitive Interactions using Singly Captured Motions</td>\n",
       "      <td>2009</td>\n",
       "      <td>Taku Komura,, Hubert P. H. Shum, Masashi Shiraishi, Shuntaro Yamazaki</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>73</td>\n",
       "      <td>Automatic Evaluation of Boxing Techniques from Captured Shadow Boxing Data</td>\n",
       "      <td>2007</td>\n",
       "      <td>Taku Komura, Jeff K. T. Tang, Howard Leung, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>74</td>\n",
       "      <td>Technical Note: Generating Realistic Fighting Scenes by Game Tree</td>\n",
       "      <td>2006</td>\n",
       "      <td>Taku Komura, Jeff K. T. Tang, Howard Leung, Hubert P. H. Shum</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>A Quadruple Diffusion Convolutional Recurrent Network for Human Motion Prediction</td>\n",
       "      <td>2022</td>\n",
       "      <td>Merryn D. Constable, Stephen Clark, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>76</td>\n",
       "      <td>A Two-Stream Recurrent Network for Skeleton-Based Human Interaction Recognition</td>\n",
       "      <td>2022</td>\n",
       "      <td>Muhammad Zeeshan Baig, Nauman Aslam, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>77</td>\n",
       "      <td>Interaction-based Human Activity Comparison</td>\n",
       "      <td>2022</td>\n",
       "      <td>Qi Feng, Hubert P. H. Shum, Shigeo Morishima</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>78</td>\n",
       "      <td>Makeup Style Transfer on Low-quality Images with Weighted Multi-scale Attention</td>\n",
       "      <td>2021</td>\n",
       "      <td>Edmond S. L. Ho, Yuan Hu, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>79</td>\n",
       "      <td>PyTorch-based Implementation of Label-aware Graph Representation for Multi-class Trajectory Prediction</td>\n",
       "      <td>2021</td>\n",
       "      <td>Asish Bera, Debotosh Bhattacharjee, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>80</td>\n",
       "      <td>A Hybrid Metaheuristic Navigation Algorithm for Robot Path Rolling Planning in an Unknown Environment</td>\n",
       "      <td>2020</td>\n",
       "      <td>Edmond S. L. Ho, Dimitrios Sakkos, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>81</td>\n",
       "      <td>A Secure Authentication Protocol for Multi-server-based e-Healthcare using a Fuzzy Commitment Scheme</td>\n",
       "      <td>2020</td>\n",
       "      <td>Edmond S. L. Ho, Daniel Organisciak, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>82</td>\n",
       "      <td>Motion Analysis of Work Conditions using Commercial Depth Cameras in Real Industrial Conditions</td>\n",
       "      <td>2020</td>\n",
       "      <td>Shanfeng Hu, Hubert P. H. Shum, Antonio Mucherino</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>83</td>\n",
       "      <td>Multiview Discriminative Marginal Metric Learning for Makeup Face Verification</td>\n",
       "      <td>2020</td>\n",
       "      <td>Qi Feng, Hubert P. H. Shum, Shigeo Morishima</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>84</td>\n",
       "      <td>3D Car Shape Reconstruction from a Single Sketch Image</td>\n",
       "      <td>2019</td>\n",
       "      <td>Edmond S. L. Ho, Shoujiang Xu, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>85</td>\n",
       "      <td>Differential Evolution Algorithm as a Tool for Optimal Feature Subset Selection in Motor Imagery EEG</td>\n",
       "      <td>2019</td>\n",
       "      <td>Qi Feng, Hubert P. H. Shum, Shigeo Morishima</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>86</td>\n",
       "      <td>Ergonomics Measurements using Kinect with a Pose Correction Framework</td>\n",
       "      <td>2018</td>\n",
       "      <td>Meng Li, Howard Leung, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>87</td>\n",
       "      <td>Experience-based Rule Base Generation and Adaptation for Fuzzy Interpolation</td>\n",
       "      <td>2018</td>\n",
       "      <td>Taku Komura, Adam Barnett, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>88</td>\n",
       "      <td>Human Action Recognition via Skeletal and Depth based Feature Fusion</td>\n",
       "      <td>2018</td>\n",
       "      <td>Lining Zhang, Hubert P. H. Shum, Ling Shao</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>89</td>\n",
       "      <td>Filtered Pose Graph for Efficient Kinect Pose Reconstruction</td>\n",
       "      <td>2017</td>\n",
       "      <td>Hubert P. H. Shum, Ronan Boulic, Michael Neff</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>90</td>\n",
       "      <td>Improving Posture Classification Accuracy for Depth Sensor-based Human Activity Monitoring in Smart Environments</td>\n",
       "      <td>2017</td>\n",
       "      <td>Worasak Rueangsirarak, Chayuti Mekurai, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>91</td>\n",
       "      <td>Inverse Dynamics Based on Occlusion-resistant Kinect Data: Is It Usable for Ergonomics?</td>\n",
       "      <td>2017</td>\n",
       "      <td>Lining Zhang, Hubert P. H. Shum, Ling Shao</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>92</td>\n",
       "      <td>Posture-based and Action-based Graphs for Boxing Skill Visualization</td>\n",
       "      <td>2017</td>\n",
       "      <td>Qi Feng, Hubert P. H. Shum, Shigeo Morishima</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>93</td>\n",
       "      <td>Unsupervised Abnormal Behaviour Detection with Overhead Crowd Video</td>\n",
       "      <td>2017</td>\n",
       "      <td>Pierre Plantard, Hubert P. H. Shum, Franck Multon</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>94</td>\n",
       "      <td>Arbitrary View Action Recognition via Transfer Dictionary Learning on Synthetic Training Data</td>\n",
       "      <td>2016</td>\n",
       "      <td>Michael Neff, Roland Geraerts, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>95</td>\n",
       "      <td>Interactive Formation Control in Complex Environments</td>\n",
       "      <td>2014</td>\n",
       "      <td>Zhiguang Liu, Howard Leung, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>96</td>\n",
       "      <td>Topology Aware Data-Driven Inverse Kinematics</td>\n",
       "      <td>2014</td>\n",
       "      <td>Taku Komura, Joseph Henry, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>97</td>\n",
       "      <td>Manufacturing Video Graphics</td>\n",
       "      <td>2012</td>\n",
       "      <td>Taku Komura, Joseph Henry, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>98</td>\n",
       "      <td>Angular Momentum Guided Motion Concatenation</td>\n",
       "      <td>2011</td>\n",
       "      <td>Taku Komura, Kevin Mackay, Hubert P. H. Shum</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>99</td>\n",
       "      <td>Physically-based Character Control in Low Dimensional Space</td>\n",
       "      <td>2011</td>\n",
       "      <td>Taku Komura, Hubert P. H. Shum, Shuntaro Yamazaki</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>Interaction Patches for Multi-Character Animation</td>\n",
       "      <td>2010</td>\n",
       "      <td>Taku Komura, Hubert P. H. Shum, Shu Takagi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>101</td>\n",
       "      <td>Simulating Interactions of Characters</td>\n",
       "      <td>2008</td>\n",
       "      <td>Taku Komura, Hubert P. H. Shum, Pranjul Yadav</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>102</td>\n",
       "      <td>Generating Realistic Fighting Scenes by Game Tree</td>\n",
       "      <td>2006</td>\n",
       "      <td>Taku Komura, Hubert P. H. Shum, Shuntaro Yamazaki</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>103</td>\n",
       "      <td>Interpreting Deep Learning based Cerebral Palsy Prediction with Channel Attention</td>\n",
       "      <td>2023</td>\n",
       "      <td>Qianhui Men, Hubert P. H. Shum</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>104</td>\n",
       "      <td>TSK Inference with Sparse Rule Bases</td>\n",
       "      <td>2016</td>\n",
       "      <td>Hubert P. H. Shum, He Wang</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>105</td>\n",
       "      <td>Real-time Physical Modelling of Character Movements with Microsoft Kinect</td>\n",
       "      <td>2014</td>\n",
       "      <td>Edmond S. L. Ho, Hubert P. H. Shum</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>106</td>\n",
       "      <td>Simulating Interactions Among Multiple Characters</td>\n",
       "      <td>2011</td>\n",
       "      <td>Edmond S. L. Ho, Hubert P. H. Shum</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>107</td>\n",
       "      <td>Simulating Interactions of Avatars in High Dimensional State Space</td>\n",
       "      <td>2008</td>\n",
       "      <td>Taku Komura, Hubert P. H. Shum</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>108</td>\n",
       "      <td>Fast Accelerometer-Based Motion Recognition with a Dual Buffer Framework</td>\n",
       "      <td>2011</td>\n",
       "      <td>Hubert P. H. Shum</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>109</td>\n",
       "      <td>Finding Repetitive Patterns in 3D Human Motion Captured Data</td>\n",
       "      <td>2009</td>\n",
       "      <td>Hubert P. H. Shum</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>110</td>\n",
       "      <td>Facial Reshaping Operator for Controllable Face Beautification</td>\n",
       "      <td>2020</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#QUESTION 1\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from itertools import takewhile\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # BeautifulSoup is the package that Python uses to scrape a google link\n",
    "url = \"https://sitescrape.awh.durham.ac.uk/comp42315_resit/publicationfull_year_characteranimation.htm\" # the google link we are scraping\n",
    "\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\") # the html content of the pages is retrieved and saved to 'soup'\n",
    "#print(soup.prettify()) #the function prettify allows us to see the layout and be able to read the html code\n",
    "\n",
    "divPerson = soup.find(\"div\", id = \"divBackground\") # tells BeautifulSoup to isolate the body of the page for scraping\n",
    "\n",
    "aAuthors = divPerson.find_all(\"p\") #tells BeautifulSoup to isolate the p tags within the isolated code\n",
    "aAuthors2 = aAuthors[0].find_all(\"a\") # tells BeautifulSoup to isolate the a tags within the second p tag ([1] indicates second). the second p tag is the list of topics\n",
    "\n",
    "urlAuthors=[] # initialise list to store the relative links for the topics\n",
    "urlAuthors.append(url[52:]) # from the google link being scraped, the relative link for the first topic is the only one not included, so we append it to the empty list here before we begin\n",
    "for aAuthors1 in aAuthors2: # for each item in the list of a tags (in each a tag is located a relative link for a different topic)\n",
    "    urlAuthors.append(aAuthors1[\"href\"]) # all relative links are added to the list just initialised.[\"href\"] indicates just the link will be appended, not the whole a tag\n",
    "    \n",
    "googleLinks =[]   # initial a list to store all google links for topic\n",
    "for relativeLink in urlAuthors:    \n",
    "    googleLink = \"https://sitescrape.awh.durham.ac.uk/comp42315_resit/\" + relativeLink\n",
    "    googleLinks.append(googleLink)\n",
    "#using the list of relative links, these have been converted to google links. A for loop has been used to iterate through each relative link.\n",
    "    \n",
    "    \n",
    "bigList1=[] # initialising a list to store titles for all publications of all topics\n",
    "bigList2=[] # initialising a list to store years for all publications of all topics\n",
    "bigList3=[] # initialising a list to store authors for all publications of all topics\n",
    "bigAuthors=[] # initialising a list to store number of authors for all publications of all topics\n",
    "    \n",
    "    \n",
    "for topicLink in googleLinks: # for each google link, in the list of google links for each topic:\n",
    "    page2 = requests.get(topicLink)\n",
    "    soup2 = BeautifulSoup(page2.content, \"html.parser\")\n",
    "    divPerson2 = soup2.find(\"div\", id = \"divBackground\") #this has isolated code, that contains sections for each type of publication\n",
    "    \n",
    "    DivBody = divPerson2.find_all(\"div\", style = \"margin-left: var(--size-marginleft)\") #this has isolated separately, all sections as directly said above\n",
    "    \n",
    "    List1=[] # initialising a list to store title for all publications of the topic currently being iterated through by the for loop\n",
    "    List2=[] # initialising a list to store year for all publications of all topic currently being iterated through by the for loop\n",
    "    UrlList=[] # initialising a list to store author of authors for all publications of all topic currently being iterated through by the for loop\n",
    "    numAuthors=[] # initialising a list to store number of authors for all publications of all topic currently being iterated through by the for loop\n",
    "    \n",
    "    for X in DivBody: # this will iterate through the separate sections of code for each type of publication \n",
    "\n",
    "        DivBody2 = X.find_all(\"div\", class_ = \"w3-cell-row\") # within each section of code as stated above, this finds all separate sections of code for each publication within the type\n",
    "\n",
    "        for I in DivBody2: # for each publication within the the type:\n",
    "\n",
    "            DivBody3 = I.find(\"div\", class_ = \"w3-container w3-cell w3-mobile w3-cell-middle\") # this isolates the code for the description of the publication\n",
    "            DivBody4 = DivBody3.find(\"span\", class_ = \"PublicationTitle\") # within that description, this isolates the code for the title\n",
    "            DivBody6 = DivBody3.find_all(\"span\", class_ = \"TextSmall\") #this finds all span tags of this class, the first one contains the year\n",
    "            \n",
    "            \n",
    "            h3s = DivBody3('a') # find all <h3> elements, this section of code extracts data for closing tags\n",
    "            Alltext=[]\n",
    "            for h3, h3next in zip(h3s, h3s[1:]):\n",
    "              # get elements in between\n",
    "              between_it = takewhile(lambda el: el is not h3next, h3.nextSiblingGenerator())\n",
    "              # extract text\n",
    "              Alltext.append(''.join(getattr(el, 'text', el) for el in between_it))\n",
    "                \n",
    "            #in Alltext, data such as publication year and some authors are stored.\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            titles = [] # titles will store author names\n",
    "            \n",
    "            for blur in Alltext: # this for loop retrieves authors from the closing tags\n",
    "                if blur.startswith(',') and blur.endswith(', ') and len(blur) > 2: # this retrieves the authors included in closing tags that are listed between commas\n",
    "                    titles.append(blur[2:-1])\n",
    "\n",
    "                if blur.endswith(' and ') and len(blur) > 5: # this retrieves authors included in the closing tags that are not the last one but are followed by and \n",
    "                    titles.append(blur[2:-5])\n",
    "    \n",
    "                if blur.startswith(' and ') and len(blur) > 5: # thsi retrieves the last author if it is included in a closing tag, and so followed by a year within the closing tag\n",
    "                    titles.append(blur.split(' in ')[0][5:])\n",
    "            \n",
    "            \n",
    "           \n",
    "            brs = DivBody3('br') # find all <h3> elements, this section of code extracts data for closing tags\n",
    "            paper=[]\n",
    "            for h3, h3next in zip(brs, brs[1:]):\n",
    "                # get elements in between\n",
    "                between_it = takewhile(lambda el: el is not h3next, h3.nextSiblingGenerator())\n",
    "                # extract text\n",
    "                paper.append(''.join(getattr(el, 'text', el) for el in between_it))\n",
    "\n",
    "                \n",
    "                \n",
    "            sentence = DivBody3.text[5:] # this section of code is retrieving the title\n",
    "            words_in_a_list = sentence.split(\" \")\n",
    "            if words_in_a_list.count(\"by\") > 1:\n",
    "                sentence = \"by\".join(sentence.split(\"by\", 2)[:2])\n",
    "            else:\n",
    "                sentence = DivBody3.text[5:].split(' by')[0]\n",
    "\n",
    "\n",
    "             \n",
    "            findTitle = I.find(\"div\", class_ = \"w3-container w3-cell w3-mobile w3-cell-middle\", style=\"padding-bottom: 24px\") \n",
    "            \n",
    "             #the if loop below retieves authors that are written in the same tag as the title of the paper\n",
    "            beforeFirstTag = findTitle.find('a').previousSibling\n",
    "            if beforeFirstTag is None:\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                beforeFirstTag = findTitle.find('br').previousSibling\n",
    "                #beforeFirstTag = findTitle.find('a').previousSibling\n",
    "                countBy = beforeFirstTag.split(\" \")\n",
    "\n",
    "                if countBy.count(\"by\") > 1:\n",
    "\n",
    "                    afterBy = \"by\".join(beforeFirstTag.split(\"by\", 2)[-1])\n",
    "                    if len(afterBy) > 1:\n",
    "\n",
    "                        if afterBy.endswith('and '):\n",
    "                            titles.append(afterBy[1:-5])\n",
    "\n",
    "                        elif afterBy.endswith(', '):\n",
    "                            titles.append(afterBy[1:-2])\n",
    "\n",
    "                else:\n",
    "                    afterBy = beforeFirstTag.split(' by')[1]\n",
    "\n",
    "                    if len(afterBy) > 1:\n",
    "\n",
    "                        if afterBy.endswith('and '):\n",
    "                            titles.append(afterBy[1:-5])\n",
    "\n",
    "                        elif afterBy.endswith(', '):\n",
    "                            titles.append(afterBy[1:-2])    \n",
    "                \n",
    "                authorlist = ', '.join(titles[:-1]) # this retrieves authors listed in closing tags by themselves and concatenates them as one string, the last element isn't an author\n",
    "\n",
    "           \n",
    "\n",
    "                                     \n",
    "                            \n",
    "\n",
    "                if Alltext is None: # if there are no 'a' tags we have to retrieve the year a different way\n",
    "                    Year = beforeFirstTag[-4:]\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                List1.append(sentence) # this appends the title of the publication being iterated through to List1\n",
    "                #needs fixing\n",
    "                List2.append(Year) # this appends the year of the publication being iterated through to List2\n",
    "                #^this is perfect\n",
    "                UrlList.append(authorlist) #this append the text outputted from the seond span tag of said class, which is in fact the list of authors names for the publication\n",
    "                #misses authors that aren't hyperlinked but works\n",
    "\n",
    "\n",
    "                countLinks = 0 # this allows the coutn to begin from zero. the count is stored as an integer\n",
    "                #for link in DivBody6[1].find_all(\"a\", href=True): # in the second span tag of said class, this finds all relative links, there is a relative link for each author  \n",
    "                    #countLinks = countLinks+1 # for each relatiev link iterated through, 1 is added to the count\n",
    "\n",
    "                for blur in titles[:-1]: # the last element in the list isn't an author\n",
    "                    countLinks = countLinks+1 # for each relatiev link iterated through, 1 is added to the count\n",
    "\n",
    "                numAuthors.append(countLinks) # when finished counting the authors, for the publication being iterated through, it appends them to the numAuthors list\n",
    "            \n",
    "            else: \n",
    "                \n",
    "                beforeFirstTag = findTitle.find('a').previousSibling\n",
    "                countBy = beforeFirstTag.split(\" \")\n",
    "\n",
    "                if countBy.count(\"by\") > 1:\n",
    "\n",
    "                    afterBy = \"by\".join(beforeFirstTag.split(\"by\", 2)[-1])\n",
    "                    if len(afterBy) > 1:\n",
    "\n",
    "                        if afterBy.endswith('and '):\n",
    "                            titles.append(afterBy[1:-5])\n",
    "\n",
    "                        elif afterBy.endswith(', '):\n",
    "                            titles.append(afterBy[1:-2])\n",
    "\n",
    "                else:\n",
    "                    afterBy = beforeFirstTag.split(' by')[1]\n",
    "\n",
    "                    if len(afterBy) > 1:\n",
    "\n",
    "                        if afterBy.endswith('and '):\n",
    "                            titles.append(afterBy[1:-5])\n",
    "\n",
    "                        elif afterBy.endswith(', '):\n",
    "                            titles.append(afterBy[1:-2])    \n",
    "\n",
    "\n",
    "                allhref = DivBody3.find_all(\"a\") #this section of code adds to the list of authors, from the href tags\n",
    "                for blur in allhref: # the last element in allref isn't an author, so will have be removed further down\n",
    "                    titles.append(blur.text) \n",
    "\n",
    "\n",
    "             \n",
    "                Year = Alltext[-1][-(len(paper[0])+14):-(len(paper[0])+10)]\n",
    "\n",
    "\n",
    "                authorlist = ', '.join(titles[:-1]) # this retrieves authors listed in closing tags by themselves and concatenates them as one string, the last element isn't an author\n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "                List1.append(sentence) # this appends the title of the publication being iterated through to List1\n",
    "                #needs fixing\n",
    "                List2.append(Year) # this appends the year of the publication being iterated through to List2\n",
    "                #^this is perfect\n",
    "                UrlList.append(authorlist) #this append the text outputted from the seond span tag of said class, which is in fact the list of authors names for the publication\n",
    "                #misses authors that aren't hyperlinked but works\n",
    "\n",
    "\n",
    "                countLinks = 0 # this allows the coutn to begin from zero. the count is stored as an integer\n",
    "                #for link in DivBody6[1].find_all(\"a\", href=True): # in the second span tag of said class, this finds all relative links, there is a relative link for each author  \n",
    "                    #countLinks = countLinks+1 # for each relatiev link iterated through, 1 is added to the count\n",
    "\n",
    "                for blur in titles[:-1]: # the last element in the list isn't an author\n",
    "                    countLinks = countLinks+1 # for each relatiev link iterated through, 1 is added to the count\n",
    "\n",
    "                numAuthors.append(countLinks) # when finished counting the authors, for the publication being iterated through, it appends them to the numAuthors list\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                        \n",
    "    for q in List1:\n",
    "        bigList1.append(q) # there is a list of titles for each google link, each google link beinga  different topic, this concatenates all these lists into one list in the order that it has iterated through\n",
    "        \n",
    "    for r in List2:\n",
    "        bigList2.append(r) # there is a list of years for each google link, each google link beinga  different topic, this concatenates all these lists into one list in the order that it has iterated through\n",
    "        \n",
    "    for t in UrlList:\n",
    "        bigList3.append(t) # there is a list of author names for each google link, each google link beinga  different topic, this concatenates all these lists into one list in the order that it has iterated through\n",
    "        \n",
    "    for v in numAuthors:\n",
    "        bigAuthors.append(v) # there is a list of number of authors for each google link, each google link beinga  different topic, this concatenates all these lists into one list in the order that it has iterated through\n",
    "\n",
    "bigList1\n",
    "bigList2\n",
    "bigList3\n",
    "bigAuthors # the last line will be printed, in this case it is list of number of authors for all publications in each topic. Printing this will enable you to see if it loks correct or not\n",
    "import pandas as pd # the pandas package within python\n",
    "\n",
    "df =  pd.DataFrame(zip(bigList1,bigList2,bigList3,bigAuthors),columns=['Title', 'Year','Authors','No. Authors']) # this means the lists can be used as collumns for a table. the data structure the lists have been zipped into is a dataframe using the pandas package\n",
    "df = df.sort_values(by=['No. Authors', 'Year', 'Title'], ascending=[False, False, True])\n",
    "df = df.drop_duplicates(subset=['Title', 'Year','Authors','No. Authors'])\n",
    "df.insert(0, 'Row number.', range(1, 1 + len(df)))\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f5fcb",
   "metadata": {},
   "source": [
    "### Write your description in the following space for Question 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1db135",
   "metadata": {},
   "source": [
    "First of all, the list of url's has to be scraped, these are appended to a list. Next we initialise the lists that will store data for every publication: 4 lists. A for loop iterates through these topic url's, each time our for loop iterates through all publications for one topic, there are separate lists that all titles, publication years, authors (and number thereof) will be appended to. \n",
    "\n",
    "The for loop iterating through publications is nested by a for loop iterating through section on the topic webpage separated by year, which is in turn nested by the for loop iterating through topic.\n",
    "\n",
    "The publication title is stored within a 'div' tag, but before its first child tag, which is either an 'a' tag or a 'br' tag. If it is an 'a' tag, all authors are either stored outside the tag but before it, or within 'a' tags, and the year is stored within a closing tag. If it is a 'br' tag, all authors are stored outside the tag but before it, and the year is stored within a 'br' tag.\n",
    "\n",
    "The 'split' function is used to separate the title from the author appending it, and also in cases where the last author is tored as a string, to separate it from 'and'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d052f1",
   "metadata": {},
   "source": [
    "## Question 2 (30 marks)\n",
    "\n",
    "You will use the scraping website to gather information related to research publications from Dr. Shumâ€™s research group. You should avoid scraping the same url information repeatedly, by storing it in a dataframe for further processing.\n",
    "\n",
    "a) Present Table 1 containing the headings: publication year, research publication title, impact factor (IF), citation count, and Similar Research items count (SRic) for the most highly-cited research publication in each year between and including 2006 and 2023. (5/30)\n",
    "\n",
    "b) Determine the proportion of document types represented in Table 1, and legibly present this information in Table 2 with headings proportion and document type, taking care with the normalisation of proportion and including all document types. (5/30)\n",
    "\n",
    "c) For each publication in Table 1, compute the impact factor mean and standard deviation of all publications with at least one shared topic with that publication, excluding the publication itself from the computation. Present the publication title, the mean impact factor of publications in shared topics, and the standard deviation in Table 3 with headings topic(s), research publication title, mean impact factor, and std. dev. impact factor. (10/30)\n",
    "\n",
    "d) Finally, in Figure 1 plot the Similar Research items count (SRic) on the ð‘¥-axis against the impact factor on the ð‘¦-axis as a scatter plot including all research publications, coloring by document type, and using a different marker shape for those listed in Table 1. (10/30)\n",
    "\n",
    "[Explain your design and highlight any features in this questionâ€™s report part of your Jupyter Notebook in no more than 300 words. ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec871117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>IF</th>\n",
       "      <th>Cit</th>\n",
       "      <th>SRIC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006</td>\n",
       "      <td>Generating Realistic Fighting Scenes by Game Tree</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>Automatic Evaluation of Boxing Techniques from Captured Shadow Boxing Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>Simulating Interactions of Avatars in High Dimensional State Space</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009</td>\n",
       "      <td>Simulating Competitive Interactions using Singly Captured Motions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>Interaction Patches for Multi-Character Animation</td>\n",
       "      <td>5.414</td>\n",
       "      <td>201</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2011</td>\n",
       "      <td>Physically-based Character Control in Low Dimensional Space</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2012</td>\n",
       "      <td>Environment Capturing with Microsoft Kinect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2013</td>\n",
       "      <td>Simulating Multiple Character Interactions with Collaborative and Adversarial Goals</td>\n",
       "      <td>4.579</td>\n",
       "      <td>112</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2014</td>\n",
       "      <td>Real-Time Posture Reconstruction for Microsoft Kinect</td>\n",
       "      <td>11.448</td>\n",
       "      <td>150</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015</td>\n",
       "      <td>Motion Adaptation for Humanoid Robots in Constrained Environments</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016</td>\n",
       "      <td>Kinect Posture Reconstruction based on a Local Mixture of Gaussian Process Models</td>\n",
       "      <td>4.579</td>\n",
       "      <td>98</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>Filtered Pose Graph for Efficient Kinect Pose Reconstruction</td>\n",
       "      <td>2.757</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018</td>\n",
       "      <td>SkillVis: A Visualization Tool for Boxing Skill Assessment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019</td>\n",
       "      <td>Differential Evolution Algorithm as a Tool for Optimal Feature Subset Selection in Motor Imagery EEG</td>\n",
       "      <td>6.954</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020</td>\n",
       "      <td>A Dual-Stream Recurrent Neural Network for Student Feedback Prediction using Kinect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021</td>\n",
       "      <td>STGAE: Spatial Temporal Graph Auto-encoder for Hand Motion Denoising</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022</td>\n",
       "      <td>A Quadruple Diffusion Convolutional Recurrent Network for Human Motion Prediction</td>\n",
       "      <td>4.685</td>\n",
       "      <td>228</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023</td>\n",
       "      <td>Interpreting Deep Learning based Cerebral Palsy Prediction with Channel Attention</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QUESTION 2 A) \n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "# YOUR CODE HERE\n",
    "from itertools import takewhile\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # BeautifulSoup is the package that Python uses to scrape a google link\n",
    "url = \"https://sitescrape.awh.durham.ac.uk/comp42315_resit/publicationfull_type_characteranimation.htm\" # the google link we are scraping\n",
    "\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\") # the html content of the pages is retrieved and saved to 'soup'\n",
    "#print(soup.prettify()) #the function prettify allows us to see the layout and be able to read the html code\n",
    "\n",
    "divPerson = soup.find(\"div\", id = \"divBackground\") # tells BeautifulSoup to isolate the body of the page for scraping\n",
    "\n",
    "aAuthors = divPerson.find_all(\"p\") #tells BeautifulSoup to isolate the p tags within the isolated code\n",
    "aAuthors2 = aAuthors[0].find_all(\"a\") # tells BeautifulSoup to isolate the a tags within the second p tag ([1] indicates second). the second p tag is the list of topics\n",
    "\n",
    "#the code below creates a list of topics to iterate through\n",
    "\n",
    "urlAuthors=[] # initialise list to store the relative links for the topics\n",
    "urlAuthors.append(url[52:]) # from the google link being scraped, the relative link for the first topic is the only one not included, so we append it to the empty list here before we begin\n",
    "for aAuthors1 in aAuthors2: # for each item in the list of a tags (in each a tag is located a relative link for a different topic)\n",
    "    urlAuthors.append(aAuthors1[\"href\"]) # all relative links are added to the list just initialised.[\"href\"] indicates just the link will be appended, not the whole a tag\n",
    "    linkText = aAuthors2[0].text\n",
    "    googleLinks =[]   # initial a list to store all google links for topic\n",
    "for relativeLink in urlAuthors:    \n",
    "    googleLink = \"https://sitescrape.awh.durham.ac.uk/comp42315_resit/\" + relativeLink\n",
    "    googleLinks.append(googleLink)\n",
    "#using the list of relative links, these have been converted to google links. A for loop has been used to iterate through each relative link.\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "yearCol=[]  # initialising a list to store years for all publications of all topics\n",
    "noCitCol=[0] * 18  # initialising a list to store number of citations for all publications of all topics\n",
    "IFcol=[0] * 18  # initialising a list to store IMPACT FACTOR for all publications of all topics\n",
    "sricCol=[0] * 18 # initialising a list to store number of SRIC for all publications of all topics\n",
    "typeCol=[0] * 18 # \n",
    "titleCol=[0] * 18\n",
    "\n",
    "yearCol = list(range(2006,2024))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for topicLink in googleLinks:  # for each google link, in the list of google links for each topic:\n",
    "    \n",
    "    #the code below finds the topic \n",
    "    \n",
    "    \n",
    "    page2 = requests.get(topicLink)\n",
    "    soup2 = BeautifulSoup(page2.content, \"html.parser\")\n",
    "    fetchTopic = soup2.find('title') # the title tag  contains text taht includes the topic currently being iterated through\n",
    "    topicText = fetchTopic.text  # the fetches the text outputed by the title tag\n",
    "    topicText = topicText[:-63]  # this removes the last 63 characters, which will be the same no matter which topic, leaving us with just the topic\n",
    "    \n",
    "    \n",
    "    divPerson2 = soup2.find(\"div\", id = \"divBackground\")  #this has isolated code, that contains sections for each type of publication\n",
    "    DivBody = divPerson2.find_all(\"div\", style = \"margin-left: var(--size-marginleft)\") # finds all div tags of this class\n",
    "    \n",
    "    \n",
    "    # below, need a for loop to go through all papers on page\n",
    "\n",
    "    for X in DivBody: # this will iterate through the separate sections of code for each type of publication \n",
    "\n",
    "        DivBody2 = X.find_all(\"div\", class_ = \"w3-cell-row\") # within each section of code as stated above, this finds all separate sections of code for each publication within the type\n",
    "\n",
    "        pubtype=X.find_previous_sibling('h2').text\n",
    "        for I in DivBody2: # for each publication within the the type: \n",
    "            \n",
    "            \n",
    "            DivBody3 = I.find(\"div\", class_ = \"w3-container w3-cell w3-mobile w3-cell-middle\") # this isolates the code for the description of the publication\n",
    "            DivBody4 = DivBody3.find(\"span\", class_ = \"PublicationTitle\") # within that description, this isolates the code for the title\n",
    "            \n",
    "            DivBody6 = DivBody3.find_all(\"span\", class_ = \"TextSmall\") #this finds all span tags of this class, the first one contains the year\n",
    "            \n",
    "            h3s = DivBody3('a') # find all <h3> elements, this section of code extracts data for closing tags\n",
    "            Alltext=[]\n",
    "            for h3, h3next in zip(h3s, h3s[1:]):\n",
    "              # get elements in between\n",
    "              between_it = takewhile(lambda el: el is not h3next, h3.nextSiblingGenerator())\n",
    "              # extract text\n",
    "              Alltext.append(''.join(getattr(el, 'text', el) for el in between_it))\n",
    "                \n",
    "            \n",
    "                  \n",
    "            #the if loop below retieves authors that are written in the same tag as the title of the paper\n",
    "            findTitle = I.find(\"div\", class_ = \"w3-container w3-cell w3-mobile w3-cell-middle\", style=\"padding-bottom: 24px\") \n",
    "              #go into each paper, need to define href firts\n",
    "\n",
    "            brs = DivBody3('br') # find all <h3> elements, this section of code extracts data for closing tags\n",
    "            paper=[]\n",
    "            for h3, h3next in zip(brs, brs[1:]):\n",
    "              # get elements in between\n",
    "                between_it = takewhile(lambda el: el is not h3next, h3.nextSiblingGenerator())\n",
    "              # extract text\n",
    "                paper.append(''.join(getattr(el, 'text', el) for el in between_it))\n",
    "\n",
    "\n",
    "                \n",
    "            DivBody4 = DivBody3.find(\"span\", class_ =\"TextSmallDefault\") # within that description, this isolates the code for the title\n",
    "            hrefPub = DivBody4.find('a')[\"href\"]\n",
    "            pubLink = \"https://sitescrape.awh.durham.ac.uk/comp42315_resit/\" + hrefPub # as we iterate through publications, we crape the relative link ofr them, convert them to a google link, and go into that to scrape the number of citations and number of links\n",
    "            page3 = requests.get(pubLink)\n",
    "            soup3 = BeautifulSoup(page3.content, \"html.parser\")\n",
    "\n",
    "            #find the number of citations first, and convert to integer\n",
    "\n",
    "            divPersonPub = soup3.find(\"div\", id = \"divBackground\")\n",
    "            divis = divPersonPub.find_all(\"div\", style = \"margin-left: var(--size-marginleft)\")\n",
    "            h1style = divis[1].find_all(\"div\")\n",
    "            test2 = h1style[2].text.split('\\xa0')[0]\n",
    "            import re\n",
    "            test2 = [int(S) for S in re.findall(r'\\d+', test2)] # this filters that etxt for only integer values, retrieving citation count\n",
    "\n",
    "\n",
    "                \n",
    "            beforeFirstTag = findTitle.find('a').previousSibling\n",
    "            if beforeFirstTag is None:\n",
    "                \n",
    "                beforeFirstTag = findTitle.find('br').previousSibling\n",
    "            \n",
    "                \n",
    "              \n",
    "                if Alltext is None: # if there are no 'a' tags the year is stored elsewhere\n",
    "                    Year = beforeFirstTag[-4:]\n",
    "                # we now have stored the year as year, but as  a string\n",
    "                Year = int(Year) \n",
    "\n",
    "                position = yearCol.index(Year)\n",
    "\n",
    "               #go into each paper, need to define href firts\n",
    "\n",
    "           \n",
    "\n",
    "                if test2[0] > noCitCol[position] or noCitCol[position] is None:\n",
    "\n",
    "                    #here we can start storing values, for a start can add number of cit\n",
    "                    noCitCol[position] = test2[0] #have stored number of cit\n",
    "\n",
    "                    #next we can add title\n",
    "\n",
    "                    sentence = DivBody3.text[5:] # this section of code is retrieving the title\n",
    "                    words_in_a_list = sentence.split(\" \")\n",
    "                    if words_in_a_list.count(\"by\") > 1:\n",
    "                        sentence = \"by\".join(sentence.split(\"by\", 2)[:2])\n",
    "                    else:\n",
    "                        sentence = DivBody3.text[5:].split(' by')[0]\n",
    "\n",
    "                    titleCol[position] = sentence\n",
    "\n",
    "                    #added title\n",
    "\n",
    "                    #next we can add impact factor\n",
    "\n",
    "                    test3 = h1style[2].text.split('\\xa0')[1]\n",
    "                    test3 = re.findall(r\"\\d+\\.\\d+\", test3) # this filters that etxt for only integer values, retrieving the 4 digit year\n",
    "\n",
    "                    IFcol[position] = float(test3[0]) # storing the IF factor\n",
    "\n",
    "                    #lastly count the SRIC \n",
    "                    soup4 = soup3.find('body')\n",
    "                    sricCol[position] = len(soup4.find_all('div',{'class': 'ImgIconSimilarDiv'}))\n",
    "                    typeCol[position] = pubtype\n",
    "                \n",
    "\n",
    "\n",
    "            else:\n",
    "                beforeFirstTag = findTitle.find('a').previousSibling\n",
    "                \n",
    "                \n",
    "              \n",
    "                Year = Alltext[-1][-(len(paper[0])+14):-(len(paper[0])+10)]\n",
    "\n",
    "                  # we now have stored the year as year, but as  a string\n",
    "                Year = int(Year) \n",
    "\n",
    "                #now need an if and (or) statement\n",
    "\n",
    "                position = yearCol.index(Year)\n",
    "\n",
    "             \n",
    "\n",
    "\n",
    "                if test2[0] > noCitCol[position] or noCitCol[position] is None:\n",
    "\n",
    "                    #here we can start storing values, for a start can add number of cit\n",
    "                    noCitCol[position] = test2[0] #have stored number of cit\n",
    "\n",
    "                    #next we can add title\n",
    "\n",
    "                    sentence = DivBody3.text[5:] # this section of code is retrieving the title\n",
    "                    words_in_a_list = sentence.split(\" \")\n",
    "                    if words_in_a_list.count(\"by\") > 1:\n",
    "                        sentence = \"by\".join(sentence.split(\"by\", 2)[:2])\n",
    "                    else:\n",
    "                        sentence = DivBody3.text[5:].split(' by')[0]\n",
    "\n",
    "                    titleCol[position] = sentence\n",
    "\n",
    "                    #added title\n",
    "\n",
    "                    #next we can add impact factor\n",
    "                    \n",
    "                    newtest = h1style[2].text.split('\\xa0')[1]\n",
    "                    newnew = re.findall(r\"\\d+\\.\\d+\", newtest) # this filters that etxt for only integer values, retrieving the 4 digit year\n",
    "                    \n",
    "                    if len(newtest) == 0:\n",
    "                        IFcol[position] = None # storing the IF factor\n",
    "                    else:\n",
    "                        \n",
    "                        IFcol[position] = float(newnew[0]) # storing the IF factor\n",
    "\n",
    "                    #lastly count the SRIC \n",
    "\n",
    "                    soup4 = soup3.find('body')\n",
    "                    sricCol[position] = len(soup4.find_all('div',{'class': 'ImgIconSimilarDiv'}))\n",
    "                    typeCol[position] = pubtype\n",
    "noCitCol             \n",
    "yearCol\n",
    "\n",
    "IFcol\n",
    "sricCol\n",
    "\n",
    "titleCol\n",
    "\n",
    "import pandas as pd # the pandas package within python\n",
    "\n",
    "df2 =  pd.DataFrame(zip(yearCol,titleCol,IFcol,noCitCol,sricCol,typeCol),columns=['Year', 'Title','IF','Cit','SRIC','type']) # this means the lists can be used as collumns for a table. the data structure the lists have been zipped into is a dataframe using the pandas package\n",
    "df4 =  pd.DataFrame(zip(yearCol,titleCol,IFcol,noCitCol,sricCol),columns=['Year', 'Title','IF','Cit','SRIC',])      \n",
    "        \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(df4.to_html()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc31691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Posters</td>\n",
       "      <td>1/18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conference Papers</td>\n",
       "      <td>5/9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Journal Papers</td>\n",
       "      <td>7/18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QUESTION 2 B) \n",
    "\n",
    "from fractions import Fraction\n",
    "import math as m\n",
    "\n",
    "\n",
    "\n",
    "def to_freq_table(data):\n",
    "    freqtable = {}\n",
    "    for key in data:\n",
    "        if key in freqtable:\n",
    "            freqtable[key] += 1\n",
    "        else:\n",
    "            freqtable[key] = 1\n",
    "    return freqtable\n",
    "occurences = to_freq_table(typeCol)\n",
    "\n",
    "\n",
    "\n",
    "answer = {k: Fraction(v/len(typeCol)).limit_denominator(18) for k,v in occurences.items()}\n",
    "df3 = pd.DataFrame({'Type':answer.keys(), 'Proportion':answer.values()})\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(df3.to_html()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af703b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QUESTION 2 C) \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # BeautifulSoup is the package that Python uses to scrape a google link\n",
    "url = \"https://sitescrape.awh.durham.ac.uk/comp42315_resit/publicationfull_type_characteranimation.htm\" # the google link we are scraping\n",
    "\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\") # the html content of the pages is retrieved and saved to 'soup'\n",
    "#print(soup.prettify()) #the function prettify allows us to see the layout and be able to read the html code\n",
    "\n",
    "divPerson = soup.find(\"div\", id = \"divBackground\") # tells BeautifulSoup to isolate the body of the page for scraping\n",
    "\n",
    "aAuthors = divPerson.find_all(\"p\") #tells BeautifulSoup to isolate the p tags within the isolated code\n",
    "aAuthors2 = aAuthors[0].find_all(\"a\") # tells BeautifulSoup to isolate the a tags within the second p tag ([1] indicates second). the second p tag is the list of topics\n",
    "\n",
    "#the code below creates a list of topics to iterate through\n",
    "\n",
    "urlAuthors=[] # initialise list to store the relative links for the topics\n",
    "urlAuthors.append(url[52:]) # from the google link being scraped, the relative link for the first topic is the only one not included, so we append it to the empty list here before we begin\n",
    "for aAuthors1 in aAuthors2: # for each item in the list of a tags (in each a tag is located a relative link for a different topic)\n",
    "    urlAuthors.append(aAuthors1[\"href\"]) # all relative links are added to the list just initialised.[\"href\"] indicates just the link will be appended, not the whole a tag\n",
    "    linkText = aAuthors2[0].text\n",
    "    googleLinks =[]   # initial a list to store all google links for topic\n",
    "for relativeLink in urlAuthors:    \n",
    "    googleLink = \"https://sitescrape.awh.durham.ac.uk/comp42315_resit/\" + relativeLink\n",
    "    googleLinks.append(googleLink)\n",
    "#using the list of relative links, these have been converted to google links. A for loop has been used to iterate through each relative link.\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "listOfLists = []\n",
    "\n",
    "\n",
    "for blur in titleCol:\n",
    "\n",
    "    listRecordingPaperLinks = []\n",
    "    \n",
    "\n",
    "    for topicLink in googleLinks:  # for each google link, in the list of google links for each topic:\n",
    "\n",
    "        #the code below finds the topic \n",
    "\n",
    "\n",
    "        page2 = requests.get(topicLink)\n",
    "        soup2 = BeautifulSoup(page2.content, \"html.parser\")\n",
    "        fetchTopic = soup2.find('title') # the title tag  contains text taht includes the topic currently being iterated through\n",
    "        topicText = fetchTopic.text  # the fetches the text outputed by the title tag\n",
    "        topicText = topicText[:-63]  # this removes the last 63 characters, which will be the same no matter which topic, leaving us with just the topic\n",
    "\n",
    "\n",
    "        divPerson2 = soup2.find(\"div\", id = \"divBackground\")  #this has isolated code, that contains sections for each type of publication\n",
    "        DivBody = divPerson2.find_all(\"div\", style = \"margin-left: var(--size-marginleft)\") # finds all div tags of this class\n",
    "\n",
    "\n",
    "        # below, need a for loop to go through all papers on page\n",
    "\n",
    "        for X in DivBody: # this will iterate through the separate sections of code for each type of publication \n",
    "\n",
    "            DivBody2 = X.find_all(\"div\", class_ = \"w3-cell-row\") # within each section of code as stated above, this finds all separate sections of code for each publication within the type\n",
    "\n",
    "\n",
    "            for I in DivBody2: # for each publication within the the type: \n",
    "\n",
    "                DivBody3 = I.find(\"div\", class_ = \"w3-container w3-cell w3-mobile w3-cell-middle\") # this isolates the code for the description of the publication\n",
    "\n",
    "                sentence = DivBody3.text[5:] # this section of code is retrieving the title\n",
    "                words_in_a_list = sentence.split(\" \")\n",
    "\n",
    "                if words_in_a_list.count(\"by\") > 1:\n",
    "                    sentence = \"by\".join(sentence.split(\"by\", 2)[:2])\n",
    "                else:\n",
    "                    sentence = DivBody3.text[5:].split(' by')[0]   \n",
    "\n",
    "\n",
    "\n",
    "                if blur == sentence:\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                    listRecordingPaperLinks.append(topicLink)\n",
    "                    \n",
    "    smallList=[]            \n",
    "                \n",
    "    for blur in listRecordingPaperLinks: \n",
    "        \n",
    "        \n",
    "        \n",
    "      \n",
    "\n",
    "        page2 = requests.get(blur)\n",
    "        soup2 = BeautifulSoup(page2.content, \"html.parser\")\n",
    "        fetchTopic = soup2.find('title') # the title tag  contains text taht includes the topic currently being iterated through\n",
    "        topicText = fetchTopic.text  # the fetches the text outputed by the title tag\n",
    "        topicText = topicText[:-63]  # this removes the last 63 characters, which will be the same no matter which topic, leaving us with just the topic\n",
    "\n",
    "\n",
    "        divPerson2 = soup2.find(\"div\", id = \"divBackground\")  #this has isolated code, that contains sections for each type of publication\n",
    "        DivBody = divPerson2.find_all(\"div\", style = \"margin-left: var(--size-marginleft)\") # finds all div tags of this class\n",
    "\n",
    "\n",
    "        # below, need a for loop to go through all papers on page\n",
    "\n",
    "        for X in DivBody: # this will iterate through the separate sections of code for each type of publication \n",
    "\n",
    "            DivBody2 = X.find_all(\"div\", class_ = \"w3-cell-row\") # within each section of code as stated above, this finds all separate sections of code for each publication within the type\n",
    "\n",
    "            pubtype=X.find_previous_sibling('h2').text\n",
    "            for I in DivBody2: # for each publication within the the type: \n",
    "\n",
    "\n",
    "                DivBody3 = I.find(\"div\", class_ = \"w3-container w3-cell w3-mobile w3-cell-middle\") # this isolates the code for the description of the publication\n",
    "                DivBody4 = DivBody3.find(\"span\", class_ = \"PublicationTitle\") # within that description, this isolates the code for the title\n",
    "\n",
    "                DivBody6 = DivBody3.find_all(\"span\", class_ = \"TextSmall\") #this finds all span tags of this class, the first one contains the year\n",
    "\n",
    "                h3s = DivBody3('a') # find all <h3> elements, this section of code extracts data for closing tags\n",
    "                Alltext=[]\n",
    "                for h3, h3next in zip(h3s, h3s[1:]):\n",
    "                  # get elements in between\n",
    "                  between_it = takewhile(lambda el: el is not h3next, h3.nextSiblingGenerator())\n",
    "                  # extract text\n",
    "                  Alltext.append(''.join(getattr(el, 'text', el) for el in between_it))\n",
    "\n",
    "\n",
    "\n",
    "                #the if loop below retieves authors that are written in the same tag as the title of the paper\n",
    "                findTitle = I.find(\"div\", class_ = \"w3-container w3-cell w3-mobile w3-cell-middle\", style=\"padding-bottom: 24px\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "               #go into each paper, need to define href firts\n",
    "\n",
    "                DivBody4 = DivBody3.find(\"span\", class_ =\"TextSmallDefault\") # within that description, this isolates the code for the title\n",
    "                hrefPub = DivBody4.find('a')[\"href\"]\n",
    "                pubLink = \"https://sitescrape.awh.durham.ac.uk/comp42315_resit/\" + hrefPub # as we iterate through publications, we crape the relative link ofr them, convert them to a google link, and go into that to scrape the number of citations and number of links\n",
    "                page3 = requests.get(pubLink)\n",
    "                soup3 = BeautifulSoup(page3.content, \"html.parser\")\n",
    "\n",
    "                divPersonPub = soup3.find(\"div\", id = \"divBackground\")\n",
    "                divis = divPersonPub.find_all(\"div\", style = \"margin-left: var(--size-marginleft)\")\n",
    "                h1style = divis[1].find_all(\"div\")\n",
    "\n",
    "                newtest = h1style[2].text.split('\\xa0')[1]\n",
    "                newnew = re.findall(r\"\\d+\\.\\d+\", newtest) # this filters that etxt for only integer values, retrieving the 4 digit year\n",
    "\n",
    "                if len(newtest) == 0:\n",
    "                        newnew = None # storing the IF factor\n",
    "                else:\n",
    "                        \n",
    "                        newnew = float(newnew[0]) # storing the IF factor\n",
    "                \n",
    "                smallList.append(newnew)\n",
    "    listOfLists.append(smallList)\n",
    "\n",
    "#the code below just ensures that there is reference size that ensures that the impact factor of the title from part A is only removed once    \n",
    "lengthOfFirstItemInListOfLists = len(listOfLists[0])\n",
    "lengthOfFirstItemInListOfLists    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddfcdffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Research Publication Title</th>\n",
       "      <th>Mean Impact Factor</th>\n",
       "      <th>Std. Dev. Impact Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generating Realistic Fighting Scenes by Game Tree</td>\n",
       "      <td>3.000938</td>\n",
       "      <td>2.997534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Automatic Evaluation of Boxing Techniques from Captured Shadow Boxing Data</td>\n",
       "      <td>3.333500</td>\n",
       "      <td>0.858231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simulating Interactions of Avatars in High Dimensional State Space</td>\n",
       "      <td>3.000938</td>\n",
       "      <td>2.997534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simulating Competitive Interactions using Singly Captured Motions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Interaction Patches for Multi-Character Animation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Physically-based Character Control in Low Dimensional Space</td>\n",
       "      <td>2.496000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Environment Capturing with Microsoft Kinect</td>\n",
       "      <td>4.457333</td>\n",
       "      <td>9.637703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Simulating Multiple Character Interactions with Collaborative and Adversarial Goals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Real-Time Posture Reconstruction for Microsoft Kinect</td>\n",
       "      <td>5.346250</td>\n",
       "      <td>17.353546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motion Adaptation for Humanoid Robots in Constrained Environments</td>\n",
       "      <td>2.217333</td>\n",
       "      <td>0.058241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kinect Posture Reconstruction based on a Local Mixture of Gaussian Process Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Filtered Pose Graph for Efficient Kinect Pose Reconstruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SkillVis: A Visualization Tool for Boxing Skill Assessment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Differential Evolution Algorithm as a Tool for Optimal Feature Subset Selection in Motor Imagery EEG</td>\n",
       "      <td>5.565500</td>\n",
       "      <td>5.498056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A Dual-Stream Recurrent Neural Network for Student Feedback Prediction using Kinect</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>STGAE: Spatial Temporal Graph Auto-encoder for Hand Motion Denoising</td>\n",
       "      <td>3.094200</td>\n",
       "      <td>6.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A Quadruple Diffusion Convolutional Recurrent Network for Human Motion Prediction</td>\n",
       "      <td>2.675364</td>\n",
       "      <td>3.100769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Interpreting Deep Learning based Cerebral Palsy Prediction with Channel Attention</td>\n",
       "      <td>4.226300</td>\n",
       "      <td>3.637967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QUESTION 2 C) CONTINUED\n",
    "\n",
    "listofMeans =[None] * 18\n",
    "listofVar =[None] * 18\n",
    "import statistics\n",
    "\n",
    "for i in listOfLists:\n",
    "    position = listOfLists.index(i) # need this for both calculating mean and variance, and also removing the effect on them from the publications listed in the title collumn\n",
    "    \n",
    "#I'm printing below to check that more than one IF factor value hasn't been removed, as requested for in the question    \n",
    "    \n",
    "    #print(len(i))\n",
    "    if IFcol[position] in i and len(listOfLists[0]) > lengthOfFirstItemInListOfLists-1:\n",
    "        i.remove(IFcol[position])\n",
    "    #print(len(i))\n",
    "    \n",
    "    listofMeans[position] = statistics.fmean(d for d in i if d is not None)\n",
    "    if i.count(None) < len(i)-1:    \n",
    "        listofVar[position] = statistics.variance(d for d in i if d is not None)\n",
    "#listofMeans\n",
    "#listofVar\n",
    "\n",
    "df5 =  pd.DataFrame(zip(titleCol,listofMeans,listofVar),columns=['Research Publication Title', 'Mean Impact Factor','Std. Dev. Impact Factor'])      \n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(df5.to_html()))        \n",
    "        \n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094b6bf",
   "metadata": {},
   "source": [
    "### Write your description in the following space for Question 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5183585e",
   "metadata": {},
   "source": [
    "\n",
    "For part A I had an 'if' statement requiring that for a publication to be stored, it must satisfy 2 conditions: for it's year to match the one being iterated through, and for the number of citations to be higher than that currently being stored for that year, unless indeed that being stored was 'None'. A for loop iterating topics encompassed this statement, wrapped in a a for loop iterating through the range of years required.\n",
    "\n",
    "For part B i had a list of type as a result of part A, as when a paper was iterated through with a higher citation count than that currently stored, the type was also recorded in a list whereby it held the same index as that of the year within its respective list. from the table for part a, the frequencies for each type could then be calculated. I chose to display the proportions as fractions, as this ensured that the ratios were exact and so would not add up to more than 1. By using 'limit-denominator' the frcations were showed in their simplest form, allowing them to be interpretable.\n",
    "\n",
    "For part C, I proceeded by initiating a list that would store a list of IF factor values for each title in partA. I had a for loop iterating through the list of titles, within this loop i intitated a list that would hold all topics for each paper. a for loop iterated through these, searching for the title in the publications on the page, if it was found the topic was appended to the list outside this loop. Outside this loop, but within the outer one for each publication, a for loop iterating through the list of topics holding one iterating through the publications on each topic page, would store a list of IF factors as items within the list outside all loops. Mean and variance could then be calculated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079d990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
